{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RELOCOR: REinforcement Learning based Optimal CORrelation for variance reduction</h1>\n",
    "\n",
    "In this notebook we give a quick start on how to use the package <tt>relocor</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import relocor\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Quick start</h2>\n",
    "\n",
    "Run a simulation with <tt>main.py</tt>; the parameters that can be modified are:\n",
    "\n",
    "<ul>\n",
    "    <li><tt>sde_name</tt>: string for the choice of the SDE to simulate. The already implemented SDE models are:\n",
    "        <ul>\n",
    "            <li><tt>'bs'</tt>: one-dimensional Black-Scholes model.</li>\n",
    "            <li><tt>'multi_bs'</tt>: multi-dimensional Black-Scholes model.</li>\n",
    "            <li><tt>'heston'</tt>: Heston model.</li>\n",
    "            <li><tt>'multi_heston'</tt>: Multi-asset Heston model.</li>\n",
    "            <li><tt>'fishing'</tt>: SDE inspired from a fish biomass evolution with fishing, <a href=\"https://arxiv.org/abs/2109.06856\">arXiv</a>.</li>\n",
    "        </ul>\n",
    "    </li> The test function is also given in the sde file.\n",
    "    <li><tt>action_name</tt>: string for the choice of the parametrization of the action. The already implemented actions are:\n",
    "        <ul>\n",
    "            <li><tt>'diag'</tt>: parametrization with a diagonal matrix.</li>\n",
    "            <li><tt>'ortho'</tt>: parametrization with a diagonal matrix and an orthogonal change of basis with blocks of 2x2 rotations.</li>\n",
    "            <li><tt>'ortho2d'</tt>: same as before but for the case where the Brownian noise has dimension 2; useful for testing and a little bit more faster for the dimension 2.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><tt>AgentClass</tt>: class of the RL algorithm for training; can be algorithm from <a href=\"https://github.com/DLR-RM/stable-baselines3/tree/master\"><tt>stable-baselines3</tt></a> adapted for box environments, such as <tt>A2C, PPO, DDPG, SAC, TD3</tt>, or our Policy Gradient agent <tt>relocor.agents.PG</tt>.</li>\n",
    "    <li><tt>batch_size</tt>, <tt>batch_eval</tt>: batch sizes for training (only for PG) and during-training-evaluation respectively.</li>\n",
    "    <li><tt>state_idxs_plot</tt> and <tt> action_idxs_plot</tt>: indexes of which dimensions of the trajectories and of the actions, respectively, to plot.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, TD3, DDPG, SAC\n",
    "\n",
    "# Parameters\n",
    "sde_name = 'multi_bs'\n",
    "action_name = 'ortho'\n",
    "AgentClass = relocor.agents.PG\n",
    "N_euler = 50\n",
    "T = 1.\n",
    "EPOCHS = 20\n",
    "batch_size = 512\n",
    "batch_eval = 512*16\n",
    "epoch_eval_freq = 1\n",
    "state_idxs_plot = [0]\n",
    "action_idxs_plot = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Environments, experiment and training</h2>\n",
    "\n",
    "Our environment <tt>env</tt> is a SDE environment where the state is the two correlated trajectories (X1, X2, t), the action is the correlation matrix and the reward is the telescopic increments of the variance. The environment is written with <a href=\"https://www.gymlibrary.dev/index.html\">OpenAI gym</a> or <a href=\"https://gymnasium.farama.org/index.html\">OpenAI gymnasium</a> environment.\n",
    "\n",
    "We also need a <tt>batch_env</tt> written in Pytorch for parallelized evaluation; <tt>batch_env</tt> describes the same environment as <tt>env</tt> but the zero'th dimension is for the batch dimension. It requires a <tt>batch_test_function</tt> (or \"payoff\") and a <tt>batch_action_param</tt>, which are the same as <tt>test_function</tt> and <tt>action_param</tt> but written in Pytorch for batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sde = relocor.sdes.multi_bs_sde\n",
    "payoff = relocor.sdes.multi_bs_payoff\n",
    "batch_payoff = relocor.sdes.multi_bs_batch_payoff\n",
    "action_param = relocor.actions.ActionOrtho(sde.dim_noise)\n",
    "batch_action_param = relocor.actions.BatchActionOrtho(sde.dim_noise)\n",
    "\n",
    "env = relocor.SDEEnvironment(\n",
    "    sde = sde,\n",
    "    T = T,\n",
    "    N_euler = N_euler,\n",
    "    test_function = payoff,\n",
    "    action_param = action_param\n",
    ")\n",
    "\n",
    "batch_env = relocor.BatchSDEEnvironment(\n",
    "    sde = sde,\n",
    "    T = T,\n",
    "    N_euler = N_euler,\n",
    "    batch_test_function = batch_payoff,\n",
    "    batch_action_param = batch_action_param\n",
    ")\n",
    "\n",
    "\n",
    "experiment = relocor.Experiment(\n",
    "    env = env, batch_env = batch_env, AgentClass = AgentClass\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constant policy can be evaluated with <tt>experiment.evaluate</tt>. Ensure that the constant policy checks the requirements of the parametrization of the action <tt>action_param</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "variance, total_reward, mean = experiment.evaluate(\n",
    "    nb_episodes=20,\n",
    "    batch_size=512*16,\n",
    "    policy_action=np.array([0., 0., 1.]))\n",
    "\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that baseline (no correlation) and antithetic policies are already implemented, depending on the parametrization of the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "variance, total_reward, mean = experiment.evaluate(\n",
    "    nb_episodes=20,\n",
    "    batch_size=512*16,\n",
    "    policy_action=experiment.env.action_param.baseline_action)\n",
    "\n",
    "print(variance)\n",
    "\n",
    "# can also use\n",
    "experiment.env.action_param.antithetic_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the chosen RL algorithm and evaluate.\n",
    "\n",
    "If there is no <tt>policy_action</tt> specified in <tt>experiment.evaluate</tt>, then the trained agent will be evaluated (must be trained before evaluation).\n",
    "\n",
    "Upon <tt>batch_eval=0</tt>, there is no callback evaluation during the training (saves time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "experiment.train(\n",
    "    total_timesteps=N_euler*EPOCHS,\n",
    "    batch_size = batch_size,\n",
    "    batch_eval = batch_eval,\n",
    "    epoch_eval_freq = epoch_eval_freq\n",
    "    )\n",
    "\n",
    "variance, total_reward, mean = experiment.evaluate(\n",
    "    nb_episodes=10,\n",
    "    batch_size=512*16)\n",
    "\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plots and save</h2>\n",
    "\n",
    "Plot the evolution of the variance during the training.\n",
    "\n",
    "Plot an example of two correlated trajectories and of the action from the trained agent; specify the dimension indexes to plot for both trajectories and action.\n",
    "\n",
    "Then save in the directory <tt>results</tt> specifying <tt>dir_name</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "experiment.plot_train_variance()\n",
    "\n",
    "experiment.run_trajectory()\n",
    "experiment.display_trajectory(state_idxs=state_idxs_plot, action_idxs=action_idxs_plot)\n",
    "\n",
    "dir_name = 'multi_bs'\n",
    "experiment.save_experiment(path='./results/{}'.format(dir_name))\n",
    "experiment.save_trajectory(path='./results/{}'.format(dir_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Custom SDE and test function</h2>\n",
    "\n",
    "In this section we enter the details to explain how to use the package with a custom SDE.\n",
    "\n",
    "A custom SDE should be written with the base class <tt>relocor.sdes.SDE</tt> and by specifying the <tt>drift</tt> and <tt>sigma</tt> (diffusion matrix) in numpy as well as the <tt>batch_drift</tt> and <tt>batch_sigma</tt> in Pytorch for the batch operations. If you don't plan to use batch environments for parallelized evaluation, you don't need to define these last two functions. Likewise, specify the test functions in numpy and Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# black scholes in one dimension\n",
    "name = 'multi Black Scholes'\n",
    "\n",
    "# dimension of the trajectory vector\n",
    "dim = 2\n",
    "\n",
    "r = 0.06\n",
    "sig = 0.3\n",
    "K = 1.\n",
    "X0 = 1.\n",
    "\n",
    "def drift(X):\n",
    "    return r*X\n",
    "\n",
    "def sigma(X):\n",
    "    return np.diag(sig*X)\n",
    "\n",
    "def batch_sigma(X):\n",
    "    return torch.diag_embed(sig*X)\n",
    "\n",
    "multi_bs_sde = relocor.sdes.SDE(\n",
    "    dim=dim,\n",
    "    dim_noise=dim, # dimension of the Brownian noise\n",
    "    drift=drift,\n",
    "    sigma=sigma,\n",
    "    X0=np.array(dim*[X0]),\n",
    "    batch_drift=drift,\n",
    "    batch_sigma=batch_sigma,\n",
    "    name=name\n",
    ")\n",
    "\n",
    "def multi_bs_payoff(X):\n",
    "    return np.mean(np.maximum(X-K,0))\n",
    "\n",
    "def multi_bs_batch_payoff(X):\n",
    "    return torch.mean(torch.relu(X-K), axis=1, keepdims=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
